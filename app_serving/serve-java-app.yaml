apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: serve-java-app
  namespace: argo
spec:
  entrypoint: main
  onExit: exit-handler
  volumes:
  - name: tks-proto-vol
    configMap:
      name: tks-proto
  arguments:
    parameters:
    ##############
    # Param "type"
    # options: build/deploy/all
    - name: type
      value: "all"
    ##################
    # Param "strategy"
    # options: rolling-update/blue-green/canary
    - name: strategy
      value: "rolling-update"
    ##################
    # Param "app_type"
    # options: spring/springboot
    - name: app_type
      value: "spring"
    - name: target_cluster_id
      value: "C011b88fa"
    - name: app_name
      value: "sample-petclinic"
    - name: namespace
      value: "{{workflow.parameters.app_name}}"
    - name: asa_id
      value: ""
    - name: asa_task_id
      value: ""
    - name: artifact_url
      value: "http://ab846aadb5b974536a0463a29ed866f5-200924269.ap-northeast-2.elb.amazonaws.com:8081/repository/my-release-repo/default/petclinic/1.0/petclinic-1.0.jar"
    - name: image_url
      value: "REGISTRY/IMAGE:TAG"
    - name: port
      value: "8080"
    #######################
    # Deploy-only params? #
    #######################
    # executable path in the image
    - name: executable_path
      value: "FILE_PATH_IN_THE_IMAGE"
    - name: profile
      value: "default"
    - name: extra_env
      value: ""
    - name: app_config
      value: ""
    - name: app_secret
      value: ""
    # resource_spec: possible values are tiny, medium, large
    - name: resource_spec
      value: "medium"
    ## Persistent volume params ##
    - name: pv_enabled
      value: false
    - name: pv_storage_class
      value: ""
    - name: pv_access_mode
      value: ""
    - name: pv_size
      value: ""
    - name: pv_mount_path
      value: ""
    # tks_info service URL
    - name: tks_info_host
      value: "tks-info.tks"
    - name: git_repo_url
      value: "github.com/openinfradev"
    - name: app-serve-template_branch
      value: "main"
    - name: harbor_pw_secret
      value: "harbor-core"

  templates:
  - name: exit-handler
    steps:
    - - name: parse-failed-step
        template: parse-failed-step
        when: "{{workflow.status}} != Succeeded"
# This expression syntax doesn't work at all as mentioned in the doc as always. Go to hell!
#    - - name: notify-failure
#        templateRef:
#          name: update-tks-asa-status
#          template: updateTksAsaStatus
#        arguments:
#          parameters:
#          - name: status
#            valueFrom:
#              expression: "steps['parse-failed-step'].outputs.parameters.step_name == build-image ? BUILD_FAILED : DEPLOY_FAILED"
#        when: "{{workflow.status}} != Succeeded"
    - - name: notify-build-failure
        templateRef:
          name: update-tks-asa-status
          template: updateTksAsaStatus
        arguments:
          parameters:
          - name: asa_task_id
            value: "{{workflow.parameters.asa_task_id}}"
          - name: status
            value: "BUILD_FAILED"
          - name: output
            value: "{{workflow.outputs.parameters.build_output_global}}"
        when: "{{workflow.status}} != Succeeded && '{{steps.parse-failed-step.outputs.parameters.step_name}}' == 'build-image'"
    - - name: notify-bluegreen-failure
        templateRef:
          name: update-tks-asa-status
          template: updateTksAsaStatus
        arguments:
          parameters:
          - name: asa_task_id
            value: "{{workflow.parameters.asa_task_id}}"
          - name: status
            value: "BLUEGREEN_FAILED"
          - name: output
            value: "{{workflow.outputs.parameters.deploy_output_global}}"
        when: "{{workflow.status}} != Succeeded && '{{steps.parse-failed-step.outputs.parameters.step_name}}' == 'deploy-app' && '{{workflow.parameters.strategy}}' == 'blue-green'"
    - - name: notify-deploy-failure
        templateRef:
          name: update-tks-asa-status
          template: updateTksAsaStatus
        arguments:
          parameters:
          - name: asa_task_id
            value: "{{workflow.parameters.asa_task_id}}"
          - name: status
            value: "DEPLOY_FAILED"
          - name: output
            value: "{{workflow.outputs.parameters.deploy_output_global}}"
        when: "{{workflow.status}} != Succeeded && '{{steps.parse-failed-step.outputs.parameters.step_name}}' == 'deploy-app' && '{{workflow.parameters.strategy}}' != 'blue-green'"

  - name: main
    steps:
    - - name: notify-build-start
        when: "{{workflow.parameters.type}} != 'deploy'"
        templateRef:
          name: update-tks-asa-status
          template: updateTksAsaStatus
        arguments:
          parameters:
          - name: asa_task_id
            value: "{{workflow.parameters.asa_task_id}}"
          - name: status
            value: "BUILDING"
          - name: output
            value: ""
    - - name: build-image
        when: "{{workflow.parameters.type}} != 'deploy'"
        template: build-image
    - - name: notify-build-success
        when: "{{workflow.parameters.type}} != 'deploy'"
        templateRef:
          name: update-tks-asa-status
          template: updateTksAsaStatus
        arguments:
          parameters:
          - name: asa_task_id
            value: "{{workflow.parameters.asa_task_id}}"
          - name: status
            value: "BUILD_SUCCESS"
          - name: output
            value: "{{steps.build-image.outputs.parameters.build_output}}"
    - - name: notify-deploy-start
        when: "{{workflow.parameters.type}} != 'build'"
        templateRef:
          name: update-tks-asa-status
          template: updateTksAsaStatus
        arguments:
          parameters:
          - name: asa_task_id
            value: "{{workflow.parameters.asa_task_id}}"
          - name: status
            value: "DEPLOYING"
          - name: output
            value: ""
    - - name: deploy-app
        when: "{{workflow.parameters.type}} != 'build'"
        template: deploy-app
    - - name: notify-deploy-success
        when: "{{workflow.parameters.type}} != 'build'"
        templateRef:
          name: update-tks-asa-status
          template: updateTksAsaStatus
        arguments:
          parameters:
          - name: asa_task_id
            value: "{{workflow.parameters.asa_task_id}}"
          - name: status
            value: "{{steps.deploy-app.outputs.parameters.status}}"
          - name: output
            value: "{{steps.deploy-app.outputs.parameters.deploy_output}}"
    - - name: update-endpoint-url
        when: "{{workflow.parameters.type}} != 'build'"
        templateRef:
          name: update-tks-asa-endpoint
          template: updateTksAsaEndpoint
        arguments:
          parameters:
          - name: asa_id
            value: "{{workflow.parameters.asa_id}}"
          - name: asa_task_id
            value: "{{workflow.parameters.asa_task_id}}"
          - name: endpoint
            value: "{{steps.deploy-app.outputs.parameters.endpoint}}"
          - name: preview_endpoint
            value: "{{steps.deploy-app.outputs.parameters.preview_endpoint}}"
          - name: helm_revision
            value: "{{steps.deploy-app.outputs.parameters.revision}}"

  #######################
  # Template Definition #
  #######################
  - name: build-image
    volumes:
    - name: varrun
      emptyDir: {}
    - name: out
      emptyDir: {}
    sidecars:
    - name: dind
      image: harbor-cicd.taco-cat.xyz/tks/docker:20.10.16-dind
      volumeMounts:
      - mountPath: /var/run
        name: varrun
      securityContext:
        privileged: true
    outputs:
      parameters:
      - name: build_output
        valueFrom:
          path: /mnt/out/build_output.log
        globalName: build_output_global
    container:
      #TODO: split worker image
      image: harbor-cicd.taco-cat.xyz/tks/appserving-worker:latest
      volumeMounts:
      - name: varrun
        mountPath: /var/run
      - name: out
        mountPath: /mnt/out
      command:
      - /bin/sh
      - '-exc'
      - |
        BUILD_LOG='/mnt/out/build_output.log'
        mkdir -p /apps && cd /apps/

        echo "Fetching app artifact.." | tee -a $BUILD_LOG

        artifact_url="{{workflow.parameters.artifact_url}}"
        curl -L -O $artifact_url 2> >(tee -a $BUILD_LOG >&2)
        artifact=${artifact_url##*\/}

        # fetch Dockerfile & manifests from git
        # NOTE: this is temporary hotfix for tks FT (diverged from main)
        git clone -b ft --single-branch https://github.com/openinfradev/app-serve-template.git

        echo "Composing Dockerfile..." | tee -a $BUILD_LOG
        app_type={{workflow.parameters.app_type}}
        if [[ "$app_type" == "spring" ]]; then
          cp ./app-serve-template/Dockerfile.spring ./Dockerfile
          sed -i "s/FILENAME/$artifact/g" ./Dockerfile
        else
          cp ./app-serve-template/Dockerfile.springboot ./Dockerfile
          sed -i "s/FILENAME/$artifact/g" ./Dockerfile
          sed -i "s/PORTNUM/{{workflow.parameters.port}}/g" ./Dockerfile
        fi
        ls -l .

        # Debug
        cat Dockerfile | tee -a $BUILD_LOG
        echo "=== End of Dockerfile ===" | tee -a $BUILD_LOG

        # Give time for the docker daemon to start in sidecar
        # TODO: It's better to check docker.sock file with busy-wait loop
        sleep 10

        echo "Building container image..." | tee -a $BUILD_LOG
        # Build docker image
        image_name="{{workflow.parameters.image_url}}"
        docker build --network host -t $image_name . 2> >(tee -a $BUILD_LOG >&2)

        # Get harbor admin password from secret
        harbor_password=Harbor12345

        # Extract registry URL from image URL
        registry_url=$(echo $image_name | awk -F/ '{ print $1 }')

        # Login to harbor registry
        docker login -u admin -p $harbor_password $registry_url

        # Push image
        echo "Pushing container image..." | tee -a $BUILD_LOG
        docker push $image_name 2> >(tee -a $BUILD_LOG >&2)

  - name: deploy-app
    volumes:
    - name: out
      emptyDir: {}
    outputs:
      parameters:
      - name: deploy_output
        valueFrom:
          path: /mnt/out/deploy_output.log
        globalName: deploy_output_global
      - name: endpoint
        valueFrom:
          path: /mnt/out/endpoint
      - name: preview_endpoint
        valueFrom:
          path: /mnt/out/preview_endpoint
      - name: revision
        valueFrom:
          path: /mnt/out/revision
      - name: status
        valueFrom:
          path: /mnt/out/deploy_status
    container:
      image: harbor-cicd.taco-cat.xyz/tks/appserving-worker:latest
      volumeMounts:
      - name: out
        mountPath: /mnt/out
      command:
      - /bin/sh
      - '-exc'
      - |
        DEPLOY_LOG='/mnt/out/deploy_output.log'
        mkdir -p /apps/

        # temporary debug
        echo "===== Application Config =====\n"
        echo "{{workflow.parameters.app_config}}"
        echo "==============================\n"

        # fetch manifests from git
        cd /apps
        # NOTE: this is temporary hotfix for tks FT (diverged from main)
        git clone -b ft --single-branch https://github.com/openinfradev/app-serve-template.git

        ##################
        # Replace values #
        ##################
        app_type={{workflow.parameters.app_type}}
        ## For legacy spring app case ##
        if [[ "$app_type" == "spring" ]]; then
          # Clone tomcat chart
          git clone https://{{workflow.parameters.git_repo_url}}/helm-charts.git

          cd /apps/app-serve-template
          # replace variable for the tomcat value-override file
          echo "Replacing variables in tomcat chart..." | tee -a $DEPLOY_LOG
          image_name="{{workflow.parameters.image_url}}"
          registry=$(echo $image_name | awk -F/ '{ print $1 }')
          image_tag=$(echo $image_name | awk -F: '{ print $2 }')

          # extract repository part using param expansion
          temp1="${image_name%:*}"
          repository="${temp1#*/}"

          sed -i "s/APP_NAME/{{workflow.parameters.app_name}}/g" ./tomcat-vo.yaml
          sed -i "s#REGISTRY#${registry}#g" ./tomcat-vo.yaml
          sed -i "s#REPOSITORY#${repository}#g" ./tomcat-vo.yaml
          sed -i "s/TAG/${image_tag}/g" ./tomcat-vo.yaml
          sed -i "s/PV_ENABLED/{{workflow.parameters.pv_enabled}}/g" ./tomcat-vo.yaml
          sed -i "s/PV_STORAGE_CLASS/{{workflow.parameters.pv_storage_class}}/g" ./tomcat-vo.yaml
          sed -i "s/PV_ACCESS_MODE/{{workflow.parameters.pv_access_mode}}/g" ./tomcat-vo.yaml
          sed -i "s/PV_SIZE/{{workflow.parameters.pv_size}}/g" ./tomcat-vo.yaml
          ## Currently, tomcat chart doesn't allow mount path override of default PVC.
          ## To override the path, extra PVC needs to be created.
          #sed -i "s#PV_MOUNT_PATH#{{workflow.parameters.pv_mount_path}}#g" ./tomcat-vo.yaml
          # Debug
          cat tomcat-vo.yaml

        ## For springboot app case ##
        else
          cd /apps/app-serve-template/chart

          # replace variable for the app
          echo "Replacing variables in helm chart..." | tee -a $DEPLOY_LOG
          image_name="{{workflow.parameters.image_url}}"
          sed -i "s#EXECUTABLE_PATH#{{workflow.parameters.executable_path}}#g" ./values.yaml
          sed -i "s/PORT_NUM/{{workflow.parameters.port}}/g" ./values.yaml
          sed -i "s/APP_NAME/{{workflow.parameters.app_name}}/g" ./values.yaml ./Chart.yaml
          sed -i "s#IMAGE_URL#${image_name}#g" ./values.yaml
          sed -i "s/PROFILE/{{workflow.parameters.profile}}/g" ./values.yaml
          sed -i "s/PV_ENABLED/{{workflow.parameters.pv_enabled}}/g" ./values.yaml
          sed -i "s/PV_STORAGE_CLASS/{{workflow.parameters.pv_storage_class}}/g" ./values.yaml
          sed -i "s/PV_ACCESS_MODE/{{workflow.parameters.pv_access_mode}}/g" ./values.yaml
          sed -i "s/PV_SIZE/{{workflow.parameters.pv_size}}/g" ./values.yaml
          sed -i "s#PV_MOUNT_PATH#{{workflow.parameters.pv_mount_path}}#g" ./values.yaml

          # write app_config to file that is embedded into configmap by helm chart later
          # the filename must be same as the one in the configmap.
          if [[ -n "{{workflow.parameters.app_config}}" ]]; then
            app_conf="{{workflow.parameters.app_config}}"
            echo "$app_conf" > config_content.yaml

            sed -i "s/CONFIGMAP_ENABLED/true/g" ./values.yaml
          else
            sed -i "s/CONFIGMAP_ENABLED/false/g" ./values.yaml
          fi

          # write secret data to file that is embedded into secret by helm chart later
          if [[ -n "{{workflow.parameters.app_secret}}" ]]; then
            app_secret="{{workflow.parameters.app_secret}}"
            echo "$app_secret" > secret_data

            sed -i "s/SECRET_ENABLED/true/g" ./values.yaml
          else
            sed -i "s/SECRET_ENABLED/false/g" ./values.yaml
          fi

          extra_env_str=""
          if [[ -n "{{workflow.parameters.extra_env}}" ]]; then
            extra_env_str="--set-json 'extraEnv={{workflow.parameters.extra_env}}'"
          fi

          # TODO: parse image tag and use it as APP_VERSION in Chart.yaml?
          #

          # Debug
          echo "===== values ====="
          cat values.yaml
        fi

        # Prepare kubeconfig
        echo "Preparing kubeconfig for target cluster..." | tee -a $DEPLOY_LOG
        KUBECONFIG_=$(kubectl get secret -n {{workflow.parameters.target_cluster_id}} {{workflow.parameters.target_cluster_id}}-kubeconfig -o jsonpath="{.data.value}" | base64 -d)
        if [[ -z "$KUBECONFIG_" ]]; then
          echo "Couldn't get kubeconfig for cluster {{workflow.parameters.target_cluster_id}}" | tee -a $DEPLOY_LOG
          exit 1
        fi

        echo "$KUBECONFIG_" > /etc/kubeconfig_temp
        chmod 0600 /etc/kubeconfig_temp
        export KUBECONFIG='/etc/kubeconfig_temp'

        ###############
        # Deploy apps #
        ###############
        echo "Starting deployment..." | tee -a $DEPLOY_LOG

        strategy={{workflow.parameters.strategy}}
        # For blue-green case, deploy rollout and preview service #
        if [[ "$strategy" == "blue-green" ]]; then
          echo "Deploying rollout resources..." | tee -a $DEPLOY_LOG

          cd /apps/app-serve-template/canary-manifests
          sed -i "s/APP_NAME/{{workflow.parameters.app_name}}/g" ./rollout.yaml
          sed -i "s/PORT_NUM/{{workflow.parameters.port}}/g" ./rollout.yaml
          kubectl apply -f ./rollout.yaml -n {{workflow.parameters.namespace}}

          # check if rollout is ready
          app_name={{workflow.parameters.app_name}}
          rollout_ready=false
          for i in `seq 1 10`
          do
            phase=$(kubectl get rollout rollout-${app_name} -n {{workflow.parameters.namespace}} -o jsonpath='{.status.phase}')
            ro_stat=$(kubectl argo rollouts status rollout-${app_name} -n {{workflow.parameters.namespace}} --timeout 2s || true)

            if [ "$phase" == "Healthy" ] && [ "$ro_stat" == "Healthy" ]; then
              echo "Rollout is ready." | tee -a $DEPLOY_LOG
              rollout_ready=true
              break
            else
              echo "Waiting for rollout to be ready.. sleeping 3 secs.."
              sleep 3
            fi
          done

          if [ "$rollout_ready" = false ]; then
            echo "Timed out waiting for rollout to be ready.." | tee -a $DEPLOY_LOG
            exit 1
          fi
        fi  # End of blue-green block

        # To make this strict, use asa_id as prefix to guarantee uniqueness of helm release.
        kubectl create ns {{workflow.parameters.namespace}} || true

        ## For legacy spring app case ##
        if [[ "$app_type" == "spring" ]]; then
          cd /apps/helm-charts/tomcat
          ## TODO: this might be temporary. Once everything is confirmed,
          ## the helm chart can be pulled from internal helm repo.
          helm upgrade --install --kubeconfig /etc/kubeconfig_temp -f /apps/app-serve-template/tomcat-vo.yaml -n {{workflow.parameters.namespace}} {{workflow.parameters.app_name}} . 2> >(tee -a $DEPLOY_LOG >&2)
        ## For springboot app case ##
        else
          cd /apps/app-serve-template/chart
          helm template test . --debug
          eval "helm upgrade --install --kubeconfig /etc/kubeconfig_temp -f values-{{workflow.parameters.resource_spec}}.yaml $extra_env_str -n {{workflow.parameters.namespace}} {{workflow.parameters.app_name}} ." 2> >(tee -a $DEPLOY_LOG >&2)
        fi

        # Just make sure helm release is deployed before status cmd
        sleep 1

        # Writing helm release info to file.
        revision=$( (helm status {{workflow.parameters.app_name}} --kubeconfig /etc/kubeconfig_temp -n {{workflow.parameters.namespace}} -o table | grep REVISION | cut -d' ' -f2) 2> >(tee -a $DEPLOY_LOG >&2) )
        
        # Cmds with pipe doesn't catch correct exit code, so we need to check it somehow.
        if [ -z "$revision" ]; then
          echo "Failed to get helm release revision. Exiting workflow.." | tee -a $DEPLOY_LOG
          exit 1
        fi

        # Debug revision number
        echo "Deployed revision number: $revision"
        echo $revision > /mnt/out/revision

        # Wait for deployment to be ready
        echo "Waiting for the deployment to be finished..." | tee -a $DEPLOY_LOG

        kubectl wait --for=condition=Available --timeout=300s -n {{workflow.parameters.namespace}} deploy/{{workflow.parameters.app_name}} 2> >(tee -a $DEPLOY_LOG >&2)

        echo "Deployment status has changed to 'Available'. Checking replicas.." | tee -a $DEPLOY_LOG

        # Check num of replicas
        ready=false
        SLEEP_INTERVAL=5
        for i in `seq 1 15`
        do
          replicas=$(kubectl get deploy/{{workflow.parameters.app_name}} -n {{workflow.parameters.namespace}} -o jsonpath='{.status.replicas}')
          available_repls=$(kubectl get deploy/{{workflow.parameters.app_name}} -n {{workflow.parameters.namespace}} -o jsonpath='{.status.availableReplicas}')

          if [ -z "$replicas" ]; then
            echo "Failed to get number of replicas. Exiting workflow.." | tee -a $DEPLOY_LOG
            exit 1
          fi

          # check if replicas == availableReplicas
          if [ "$replicas" == "$available_repls" ]; then
            echo "All replicas are available. Deployment is successful!" | tee -a $DEPLOY_LOG
            ready=true
            break
          fi
          sleep $SLEEP_INTERVAL
        done  # End of FOR loop #

        if [ "$ready" = false ]; then
          echo "Timed out waiting for deployment to be done.." | tee -a $DEPLOY_LOG
          #echo "BLUEGREEN_FAILED" > /mnt/out/deploy_status
          exit 1
        fi

        ## For blue-green case, check if rollout is completed ##
        if [[ "$strategy" == "blue-green" ]]; then
          rollout_finished=false
          for i in `seq 1 10`
          do
            # Check if rollout is completed
            if (kubectl argo rollouts status rollout-${app_name} -n {{workflow.parameters.namespace}} --timeout 2s | grep Paused); then
              echo "Rollout is completed. Waiting for user's approval.." | tee -a $DEPLOY_LOG
              rollout_finished=true
              break
            else
              echo "Waiting for rollout to be finished.. sleeping 3 secs.."
              sleep 3
            fi
          done

          if [ "$rollout_finished" = false ]; then
            echo "Timed out waiting for rollout to be completed.." | tee -a $DEPLOY_LOG
            #echo "BLUEGREEN_FAILED" > /mnt/out/deploy_status
            exit 1
          fi

          # Wait 5 secs for the endpoint to be ready
          sleep 5

          # Writing preview-svc endpoint to file
          kubectl get svc ${app_name}-preview -n {{workflow.parameters.namespace}} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' > /mnt/out/preview_endpoint

          # Write deployment status to file for next step
          echo "WAIT_FOR_PROMOTE" > /mnt/out/deploy_status
        else
          ## For rolling-update ##
          # Prevent output param from being null
          echo "N/A" > /mnt/out/preview_endpoint

          # Write deployment status to file for next step
          echo "DEPLOY_SUCCESS" > /mnt/out/deploy_status
        fi

        # Wait 5 secs for the endpoint to be ready
        # TODO: fix this temp sleep cmd to busy-wait loop
        sleep 5

        # Writing endpoint to file
        kubectl get svc {{workflow.parameters.app_name}} -n {{workflow.parameters.namespace}} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' > /mnt/out/endpoint

  - name: parse-failed-step
    volumes:
    - name: out
      emptyDir: {}
    outputs:
      parameters:
      - name: step_name
        valueFrom:
          path: /mnt/out/failed_step_name.txt
    script:
      image: harbor-cicd.taco-cat.xyz/tks/python:alpine3.8
      volumeMounts:
      - name: out
        mountPath: /mnt/out
      command: [python]
      source: |
        import time
        import json

        wf_failures = {{workflow.failures}}

        # convert string to list
        wf_failure_list = json.loads(wf_failures)
        print(type(wf_failure_list))

        failed_step=''
        for step in wf_failure_list:
          print("Processing str: {}".format(step))
          # step is 'dict' type now.
          if step["templateName"] == 'build-image' or step["templateName"] == 'deploy-app':
            print("found failed step {}".format(step["templateName"]))
            failed_step = step["templateName"]
            break

        if failed_step:
          print("Writing failed step name '{}' to file...".format(failed_step))
          with open('/mnt/out/failed_step_name.txt', 'w') as f:
            f.write(failed_step)
        else:
          print("Couldn't find failed step name!")
          exit(1)
