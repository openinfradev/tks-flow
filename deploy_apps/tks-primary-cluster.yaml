apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: tks-primary-cluster
  namespace: argo
spec:
  entrypoint: set-primary-cluster
  arguments:
    parameters:
    ##########################
    # needed by sub-systems (sub-template)
    ##########################
    - name: site_name  # primary cluster
      value: site3
    - name: revision
      value: "main"
    - name: app_prefix
      value: "{{workflow.parameters.site_name}}"
    - name: repository_base
      value: https://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@github.com/overmon/
    - name: github_account
      value: "decapod10"
    - name: object_store
      value: "s3"
    - name: alert_tks
      value: "NA"
    - name: alert_slack
      value: "NA"

    ##########################
    # For tks-info task #
    ##########################
    - name: tks_api_url
      value: "http://tks-api-dev.taco-cat.xyz:9110"
    - name: organization_id
      value: "o086tb3zc"
    # 아마도 site_name과 동일하게 사용하면 될듯하다.
    - name: cluster_id
      value: "{{workflow.parameters.site_name}}"
    - name: base_repo_branch
      value: develop
    - name: cloud_account_id
      value: ""

  volumes:
  - name: tks-proto-vol
    configMap:
      name: tks-proto
  - name: kubeconfig-adm
    secret:
      secretName: tks-admin-kubeconfig-secret

  templates:
  - name: set-primary-cluster
    synchronization:
      mutex:
        name: primary-cluster-{{workflow.parameters.organization_id}}
    inputs:
      parameters:
      - name: primary_cluster
      - name: member_clusters
    steps:
    - - name: update-status-mainternance
        template: sub-update-tks-status
        arguments:
          parameters:
          - name: tks_status
            value: MAINTERNANCE
      # - name: get-clusters-in-contract
      #   template: sub-get-cluster

    - - name: set-primary-cluster-on-tks-info
        template: sub-set-primary-cluster-on-tks-info
        arguments:
          parameters:
          - name: cluster_id
            value: "{{workflow.parameters.cluster_id}}"

    # TODO: 전체 완성을 위해서는 아래내역을 구현하여 동적인 bucket을 만드는 방식으로 구현해야 하지만
    #     5월 오픈전 가능한 형상을 위해 협의한 바(아래)에 따라 본부분은 기존 준비됀 것을 사용하는 것으로 구현하고 추후 수정하다.
    #       1. 사용자가 생성하는 첫번째 클러스터는 primary cluster
    #       2. primary cluster는 계약이 종료되기 전까지 임의 삭제불가
    #       3. 개별 클러스터에서 수행되는 모니터링은 없고 계약단위에서 수행되어야 함
    # - - name: prepare-bucket
    #     templateRef:
    #       name: create-application
    #       template: installApps
    #     arguments:
    #       parameters:
    #       - name: list
    #         value: |
    #           [
    #             { "app_group": "tks-cluster", "path": "lma-bucket", "namespace": "taco-system", "target_cluster": "" }
    #           ]

    # 개별 클러스터에 설치됀 loki와 grafana는 지우자
    # - - name: remove-individual-loki-and-grafana
    #     template: sub-remove-individual-loki-and-grafana
    #     arguments:
    #       parameters:
    #       - name: target_clusters
    #         value: '{{steps.get-clusters-in-contract.outputs.parameters.primary_cluster}} {{steps.get-clusters-in-contract.outputs.parameters.member_clusters}}'

    - - name: federation-components-preinstall-for-minio
        templateRef:
          name: create-application
          template: installApps
        arguments:
          parameters:
          - name: list
            value: |
              [
                { "app_group": "lma", "path": "minio", "namespace": "lma", "target_cluster": "" },
                { "app_group": "lma", "path": "loki", "namespace": "lma", "target_cluster": "" }
              ]
        when: "{{workflow.parameters.object_store}} == minio"

    - - name: loki-use-s3
        template: loki-use-s3
        arguments:
          parameters:
          - name: primary_cluster
            value: '{{inputs.parameters.primary_cluster}}'
          - name: member_clusters
            value: '{{inputs.parameters.member_clusters}}'
          - name: github_account
            value: "{{ workflow.parameters.github_account }}"
          - name: base_repo_branch
            value: "{{ workflow.parameters.base_repo_branch }}"
        when: "{{workflow.parameters.object_store}} == s3"

    - - name: change-target
        template: change-logging-target
        arguments:
          parameters:
          - name: primary_cluster
            value: '{{inputs.parameters.primary_cluster}}'
          - name: member_clusters
            value: '{{inputs.parameters.member_clusters}}'

    - - name: federation-components-postinstall
        templateRef:
          name: create-application
          template: installApps
        arguments:
          parameters:
          - name: list
            value: |
              [
                { "app_group": "lma", "path": "thanos-config", "namespace": "lma", "target_cluster": "" },
                { "app_group": "lma", "path": "thanos", "namespace": "lma", "target_cluster": "" },
                { "app_group": "lma", "path": "grafana", "namespace": "lma", "target_cluster": "" }
              ]

    - - name: update-status-done
        template: sub-update-tks-status
        arguments:
          parameters:
          - name: tks_status
            value: Running

  - name: change-logging-target
    inputs:
      parameters:
      - name: primary_cluster
      - name: member_clusters
    steps:
    - - name: change-target
        template: sub-change-logging-target
        arguments:
          parameters:
          - name: primary_cluster
            value: '{{inputs.parameters.primary_cluster}}'
          - name: member_clusters
            value: '{{inputs.parameters.member_clusters}}'

    - - name: wait-for-rendering-to-finish-modified-cluster
        templateRef:
          name: wait-for-rendering-to-finish
          template: main
        arguments:
          parameters:
          - name: cluster_id
            value: "{{ workflow.parameters.github_account }}/{{ item }}"
        withParam: "{{ steps.change-target.outputs.parameters.modified_cluster_list}}"

  - name: loki-use-s3
    inputs:
      parameters:
          - name: primary_cluster
          - name: member_clusters
          - name: github_account
          - name: base_repo_branch

    steps:
    - - name: pre-change-target
        template: sub-pre-change-logging-target
        arguments:
          parameters:
          - name: primary_cluster
            value: '{{inputs.parameters.primary_cluster}}'
          - name: member_clusters
            value: '{{inputs.parameters.member_clusters}}'

    - - name: wait-for-rendering-to-finish-pre-modified-cluster
        templateRef:
          name: wait-for-rendering-to-finish
          template: main
        arguments:
          parameters:
          - name: cluster_id
            value: "{{ workflow.parameters.github_account }}/{{ item }}"
        withParam: "{{ steps.pre-change-target.outputs.parameters.modified_cluster_list}}"

    - - name: federation-components-preinstall-for-s3
        templateRef:
          name: create-application
          template: installApps
        arguments:
          parameters:
          - name: list
            value: |
              [
                { "app_group": "lma", "path": "lma-bucket", "namespace": "taco-system", "target_cluster": "" },
                { "app_group": "lma", "path": "loki", "namespace": "lma", "target_cluster": "" }
              ]

  - name: update-eps-for-thanos
    inputs:
      parameters:
      - name: primary_cluster
      - name: member_clusters
    steps:
    - - name: change-thanos-sidecar
        template: sub-change-thanos-sidecar
        arguments:
          parameters:
          - name: primary_cluster
            value: '{{inputs.parameters.primary_cluster}}'

    - - name: wait-for-rendering-to-finish-this-cluster
        templateRef:
          name: wait-for-rendering-to-finish
          template: main
        arguments:
          parameters:
          - name: cluster_id
            value: "{{ workflow.parameters.github_account }}/{{ workflow.parameters.cluster_id }}"

    - - name: sync-organization-changes
        template: sub-sync-organization-changes
        arguments:
          parameters:
          - name: primary_cluster
            value: '{{inputs.parameters.primary_cluster}}'
          - name: member_clusters
            value: '{{inputs.parameters.member_clusters}}'

    - - name: wait-for-rendering-to-finish-changed-clusters
        templateRef:
          name: wait-for-rendering-to-finish
          template: main
        arguments:
          parameters:
          - name: cluster_id
            value: "{{ workflow.parameters.github_account }}/{{steps.sync-organization-changes.outputs.parameters.changed_primary_id}}"
        when: "'{{steps.sync-organization-changes.outputs.parameters.changed_primary_id}}' != NONE"

  #######################
  # Template Definition #
  #######################
  - name: sub-update-tks-status
    inputs:
      parameters:
      - name: tks_status
    container:
      name: update-tks-status
      image: harbor.taco-cat.xyz/tks/hyperkube:v1.18.6
      command:
      - /bin/bash
      - '-c'
      - |
        echo "check tks status"
        echo "if status is not running then throw exception~~"
        echo "connect tks-info server and change the {{workflow.parameters.organization_id}} status to mode:{{inputs.parameters.tks_status}}"
    activeDeadlineSeconds: 900
    retryStrategy:
      limit: 2

  # function sub-pre-change-logging-target
  # 1. Change endpoint of fluentbit-output (all in org.)
  # 2. Change endpoint of thanos-sidecar in prometheus-pod (all in org.)
  - name: sub-pre-change-logging-target
    inputs:
      parameters:
      - name: primary_cluster
      - name: member_clusters
    container:
      name: logging-target-changer
      image: harbor.taco-cat.xyz/tks/shyaml_jq_yq_kubectl_python:3.11
      command:
      - /bin/bash
      - '-c'
      - |
        #/bin/bash

        set -ex

        function log() {
          level=$1
          msg=$2
          date=$(date '+%F %H:%M:%S')
          echo "[$date] $level  $msg"
        }

        current_cluster={{workflow.parameters.cluster_id}}
        primary_cluster={{inputs.parameters.primary_cluster}}
        member_clusters="{{inputs.parameters.member_clusters}}"
        empty_char=

        if [ -z ${primary_cluster} ] || [[ "${primary_cluster}" == "$empty_char" ]]; then
          primary_cluster=${current_cluster}
        fi

        S3_SERVICE="s3://ap-northeast-2"
        cp /kube/value ~/kubeconfig_adm
        export KUBECONFIG=~/kubeconfig_adm

        #################
        # updates
        #################
        GIT_ACCOUNT={{workflow.parameters.github_account}}
        if  [[ $GIT_SVC_URL == https://* ]]; then
          repository_base=https://${TOKEN//[$'\t\r\n ']}@${GIT_SVC_URL/http:\/\//}/${GIT_ACCOUNT}/
        else
          repository_base=http://${TOKEN//[$'\t\r\n ']}@${GIT_SVC_URL/http:\/\//}/${GIT_ACCOUNT}/
        fi

        # Use AWS STS temporary security credential if multi-tenancy
        if [ -z $CLOUD_ACCOUNT_ID ]; then
          iamPrefix="arn:aws:iam::$(kubectl get secret -n argo awsconfig-secret -ojsonpath='{.data.AWS_ACCOUNT_ID}' | base64 -d):role"
        else
          ROLE_ARN=$(kubectl get awsri $CLOUD_ACCOUNT_ID-account-role -ojsonpath='{.spec.roleARN}')
          iamPrefix=${ROLE_ARN%/*}
        fi

        for member in $member_clusters
        do
          # 1. endpoint of fb on eachcluster
          log "INFO" "##### change the loki target to $LOKI_HOST:$LOKI_PORT and $S3_SERVICE (the current target is ${member})"
          [ -d ${member} ] || git clone ${repository_base}${member}
          cd ${member}

          yq -i e "del(.charts[] | select(.name == \"thanos-config\").override.objectStorage)" ${member}/lma/site-values.yaml
          yq -i e ".charts |= map(select(.name == \"thanos-config\").override.objectStorage.type=\"s3\")" ${member}/lma/site-values.yaml
          yq -i e ".charts |= map(select(.name == \"thanos-config\").override.objectStorage.rawConfig.endpoint=\"s3.ap-northeast-2.amazonaws.com\")" ${member}/lma/site-values.yaml
          yq -i e ".charts |= map(select(.name == \"thanos-config\").override.objectStorage.rawConfig.region=\"ap-northeast-2\")" ${member}/lma/site-values.yaml
          yq -i e ".charts |= map(select(.name == \"thanos-config\").override.objectStorage.rawConfig.bucket=\"${primary_cluster}-tks-thanos\")" ${member}/lma/site-values.yaml
          yq -i e ".charts |= map(select(.name == \"thanos-config\").override.objectStorage.rawConfig.signature_version2=false)" ${member}/lma/site-values.yaml
          yq -i e ".charts |= map(select(.name == \"thanos-config\").override.objectStorage.rawConfig.aws_sdk_auth=true)" ${member}/lma/site-values.yaml
          yq -i e  ".global.clusterName=\"${member}\"" ${member}/lma/site-values.yaml

          yq -i e "del(.charts[] | select(.name == \"loki\").override.loki.storageConfig.aws)" ${member}/lma/site-values.yaml
          yq -i e ".charts |= map(select(.name == \"loki\").override.loki.storageConfig.aws.s3=\"s3://ap-northeast-2\")" ${member}/lma/site-values.yaml
          yq -i e ".charts |= map(select(.name == \"loki\").override.loki.storageConfig.aws.dynamodb.dynamodb_url=\"dynamodb://ap-northeast-2\")" ${member}/lma/site-values.yaml
          yq -i e ".charts |= map(select(.name == \"loki\").override.loki.storageConfig.aws.bucketnames=\"${primary_cluster}-tks-loki\")" ${member}/lma/site-values.yaml

          if [ `kubectl get AWSManagedMachinePool -n ${member} ${member}-mp-taco --ignore-not-found=true | grep -v NAME | wc -l ` -eq 1 ]; then
            if [ -z $iamRoles ]; then
              iamRoles="\"${iamPrefix}/$(kubectl get AWSManagedMachinePool -n ${member} ${member}-mp-taco  -o jsonpath='{.spec.roleName}')\""
            else
              iamRoles="${iamRoles}, \"${iamPrefix}/$(kubectl get AWSManagedMachinePool -n ${member} ${member}-mp-taco  -o jsonpath='{.spec.roleName}')\""
            fi
          elif [ `kubectl get AWSMachinePool -n ${member} ${member}-mp-taco --ignore-not-found=true | grep -v NAME | wc -l ` -eq 1 ]; then
            if [ -z $iamRoles ]; then
              iamRoles="\"${iamPrefix}/nodes.cluster-api-provider-aws.sigs.k8s.io\""
            else
              iamRoles="${iamRoles}, \"${iamPrefix}/nodes.cluster-api-provider-aws.sigs.k8s.io\""
            fi
          else
            log "WARN" "Cluster(${member}) has no node role for taco-lma"
          fi

          cd -
        done
        yq -i e  ".global.tksIamRoles=[${iamRoles}]" ${primary_cluster}/${primary_cluster}/lma/site-values.yaml

        git config --global user.name "tks"
        git config --global user.email "tks@sktelecom.com"

        for member in $member_clusters
        do
          cd ${member}
          if [[ `git status --porcelain` ]]; then
            log "INFO" "##### commit changes on ${member} to use s3"
            cmessage="changes on ${member} to use s3"
            git add ${member}/lma/site-values.yaml
            git commit -m "change loki and thanos endpoints. (by set-primary workflow)" -m "$cmessage"
            git push
            modified_clusters="${modified_clusters} ${member}"
            # echo -n "${member} " >>  /mnt/out/modified_cluster_list.txt
          else
            log "INFO" "No change on the cluster ${member}"
          fi
          cd -
          rm -rf ${member}
        done
        jq -n '$ARGS.positional' --args $modified_clusters >  /mnt/out/modified_cluster_list.txt

      env:
      - name: OBJECT_STORE
        value: "{{workflow.parameters.object_store}}"
      envFrom:
      - secretRef:
          name: "git-svc-token"
      volumeMounts:
      - name: kubeconfig-adm
        mountPath: "/kube"
      - name: out
        mountPath: /mnt/out
    volumes:
    - name: out
      emptyDir: {}
    outputs:
      parameters:
      - name: modified_cluster_list
        valueFrom:
          path: /mnt/out/modified_cluster_list.txt
    activeDeadlineSeconds: 900

  - name: sub-change-logging-target
    inputs:
      parameters:
      - name: primary_cluster
      - name: member_clusters
    container:
      name: logging-target-changer
      image: harbor.taco-cat.xyz/tks/shyaml_jq_yq_kubectl_python:3.11
      command:
      - /bin/bash
      - '-c'
      - |
        #/bin/bash

        set -ex

        function log() {
          level=$1
          msg=$2
          date=$(date '+%F %H:%M:%S')
          echo "[$date] $level  $msg"
        }

        current_cluster={{workflow.parameters.cluster_id}}
        primary_cluster={{inputs.parameters.primary_cluster}}
        member_clusters="{{inputs.parameters.member_clusters}}"
        empty_char=

        if [ -z ${primary_cluster} ] || [[ "${primary_cluster}" == "$empty_char" ]]; then
          primary_cluster=${current_cluster}
        fi

        #################
        # Check endpoints from the primary_cluster
        #################
        primary_kube_secret=$(kubectl get secret -n ${primary_cluster} ${primary_cluster}-tks-kubeconfig -o jsonpath="{.data.value}" | base64 -d)
        # echo -e "primary_kube_secret:\n$primary_kube_secret" | head -n 5
        cat <<< "$primary_kube_secret" > kubeconfig
        LOKI_SERVICE=$(kubectl get secret -n ${primary_cluster} tks-endpoint-secret -o jsonpath='{.data.loki}'| base64 -d )

        if [[ "$LOKI_SERVICE" == "" ]]; then
          while [ -z $(kubectl --kubeconfig=kubeconfig get svc -n lma loki-loki-distributed-gateway -o jsonpath="{.status.loadBalancer.ingress[*].hostname}") ]
          do
            if [ "$(kubectl --kubeconfig=kubeconfig get svc -n lma loki-loki-distributed-gateway -o jsonpath="{.spec.type}")" -neq "LoadBalancer" ]; then
              log "FAIL" "The infras on primary are not cofigured properly.(No LoadBalancer)"
              exit -1
            fi

            echo "Waiting for generating the loadbalancer of LOKI(3s)"
            sleep 3
          done

          LOKI_HOST=$(kubectl --kubeconfig=kubeconfig get svc -n lma loki-loki-distributed-gateway -o jsonpath="{.status.loadBalancer.ingress[0].hostname}")
          LOKI_PORT=$(kubectl --kubeconfig=kubeconfig get svc -n lma loki-loki-distributed-gateway -o jsonpath="{.spec.ports[0].port}")
        else
          LOKI_HOST=$(echo $LOKI_SERVICE | awk -F : '{print $1}')
          LOKI_PORT=$(echo $LOKI_SERVICE | awk -F : '{print $2}')
          if [[ "$LOKI_PORT" == "" ]]; then
            LOKI_PORT=80
          fi
        fi

        if [[ "$OBJECT_STORE" == "minio" ]]; then
          S3_SERVICE=$(kubectl get secret -n ${primary_cluster} tks-endpoint-secret -o jsonpath='{.data.minio}'| base64 -d )
          if [[ "$S3_SERVICE" == "" ]]; then
            S3_HOST=$(kubectl --kubeconfig=kubeconfig get svc -n lma minio -o jsonpath="{.status.loadBalancer.ingress[0].hostname}")
            S3_PORT=$(kubectl --kubeconfig=kubeconfig get svc -n lma minio -o jsonpath="{.spec.ports[0].port}")
            S3_SERVICE=${S3_HOST}:${S3_PORT}
          fi
        fi

        #################
        # updates
        #################
        GIT_ACCOUNT={{workflow.parameters.github_account}}
        if  [[ $GIT_SVC_URL == https://* ]]; then
          repository_base=https://${TOKEN//[$'\t\r\n ']}@${GIT_SVC_URL/http:\/\//}/${GIT_ACCOUNT}/
        else
          repository_base=http://${TOKEN//[$'\t\r\n ']}@${GIT_SVC_URL/http:\/\//}/${GIT_ACCOUNT}/
        fi

        for member in $member_clusters
        do
          # 1. endpoint of fb on eachcluster
          log "INFO" "##### change the loki target to $LOKI_HOST:$LOKI_PORT and $S3_SERVICE (the current target is ${member})"
          [ -d ${member} ] || git clone ${repository_base}${member}
          cd ${member}

          yq -i e  ".global.lokiHost=\"${LOKI_HOST}\"" ${member}/lma/site-values.yaml
          yq -i e  ".global.lokiPort=\"${LOKI_PORT}\"" ${member}/lma/site-values.yaml
          if [[ "$OBJECT_STORE" == "minio" ]]; then
            yq -i e  ".global.s3Service=\"${S3_SERVICE}\"" ${member}/lma/site-values.yaml
          fi

          yq -i e  ".global.clusterName=\"${member}\"" ${member}/lma/site-values.yaml

          # 2. grafana datasource on primary_cluster
          if [[ "${member}" == "${primary_cluster}" ]]; then
            yq -i e  ".global.grafanaDatasourceMetric=\"thanos-query.lma:9090\"" ${member}/lma/site-values.yaml
            yq -i e  ".global.TksWebhookUrl=\"{{workflow.parameters.alert_tks}}\"" ${member}/lma/site-values.yaml
            yq -i e  ".global.SlackUrl=\"{{workflow.parameters.alert_slack}}\"" ${member}/lma/site-values.yaml
          else
            yq -i e  ".global.grafanaDatasourceMetric=\"lma-prometheus.lma:9090\"" ${member}/lma/site-values.yaml
          fi

          cd -
        done

        git config --global user.name "tks"
        git config --global user.email "tks@sktelecom.com"

        for member in $member_clusters
        do
          cd ${member}
          if [[ `git status --porcelain` ]]; then
            log "INFO" "##### commit changes on ${member} to $LOKI_HOST:$LOKI_PORT and $S3_SERVICE"
            if [[ "$OBJECT_STORE" == "minio" ]]; then
              cmessage="the loki to $LOKI_HOST:$LOKI_PORT and grafana to $S3_SERVICE (cluster ${member})"
            else
              cmessage="the loki to $LOKI_HOST:$LOKI_PORT (cluster ${member})"
            fi
            git add ${member}/lma/site-values.yaml
            git commit -m "change loki and thanos endpoints. (by set-primary workflow)" -m "$cmessage"
            git push
            modified_clusters="${modified_clusters} ${member}"
            # echo -n "${member} " >>  /mnt/out/modified_cluster_list.txt
          else
            log "INFO" "No change on the cluster ${member}"
          fi
          cd -
          rm -rf ${member}
        done
        jq -n '$ARGS.positional' --args $modified_clusters >  /mnt/out/modified_cluster_list.txt

      env:
      - name: OBJECT_STORE
        value: "{{workflow.parameters.object_store}}"
      envFrom:
      - secretRef:
          name: "git-svc-token"
      volumeMounts:
      - name: out
        mountPath: /mnt/out
    volumes:
    - name: out
      emptyDir: {}
    outputs:
      parameters:
      - name: modified_cluster_list
        valueFrom:
          path: /mnt/out/modified_cluster_list.txt
    activeDeadlineSeconds: 900

  - name: sub-change-thanos-sidecar
    inputs:
      parameters:
      - name: primary_cluster
    container:
      name: logging-target-changer
      image: harbor.taco-cat.xyz/tks/shyaml_jq_yq_kubectl_python:3.11
      command:
      - /bin/bash
      - '-c'
      - |
        #/bin/bash

        set -ex

        function log() {
          level=$1
          msg=$2
          date=$(date '+%F %H:%M:%S')
          echo "[$date] $level  $msg"
        }

        cp /kube/value ~/kubeconfig_adm
        export KUBECONFIG=~/kubeconfig_adm

        current_cluster={{workflow.parameters.cluster_id}}
        primary_cluster={{inputs.parameters.primary_cluster}}
        empty_char=

        if [ -z ${primary_cluster} ] || [[ "${primary_cluster}" == "$empty_char" ]]; then
          primary_cluster=${current_cluster}
        fi

        S3_SERVICE=$(kubectl get secret -n ${primary_cluster} tks-endpoint-secret -o jsonpath='{.data.minio}'| base64 -d )
        if [[ "$S3_SERVICE" != "" ]]; then
          echo "This site uses the predefined loki and static object stores."
          exit 0
        fi

        S3_SERVICE="s3://ap-northeast-2"

        GIT_ACCOUNT={{workflow.parameters.github_account}}
        if  [[ $GIT_SVC_URL == https://* ]]; then
          repository_base=https://${TOKEN//[$'\t\r\n ']}@${GIT_SVC_URL/http:\/\//}/${GIT_ACCOUNT}/
        else
          repository_base=http://${TOKEN//[$'\t\r\n ']}@${GIT_SVC_URL/http:\/\//}/${GIT_ACCOUNT}/
        fi

        log "INFO" "##### configure thanos object storage (the current target is ${current_cluster})"
        [ -d ${current_cluster} ] || git clone ${repository_base}${current_cluster}
        cd ${current_cluster}

        yq -i e "del(.charts[] | select(.name == \"thanos-config\").override.objectStorage)" ${current_cluster}/lma/site-values.yaml
        yq -i e ".charts |= map(select(.name == \"thanos-config\").override.objectStorage.type=\"s3\")" ${current_cluster}/lma/site-values.yaml
        yq -i e ".charts |= map(select(.name == \"thanos-config\").override.objectStorage.rawConfig.endpoint=\"s3.ap-northeast-2.amazonaws.com\")" ${current_cluster}/lma/site-values.yaml
        yq -i e ".charts |= map(select(.name == \"thanos-config\").override.objectStorage.rawConfig.region=\"ap-northeast-2\")" ${current_cluster}/lma/site-values.yaml
        yq -i e ".charts |= map(select(.name == \"thanos-config\").override.objectStorage.rawConfig.bucket=\"${primary_cluster}-tks-thanos\")" ${current_cluster}/lma/site-values.yaml
        yq -i e ".charts |= map(select(.name == \"thanos-config\").override.objectStorage.rawConfig.signature_version2=false)" ${current_cluster}/lma/site-values.yaml
        yq -i e ".charts |= map(select(.name == \"thanos-config\").override.objectStorage.rawConfig.aws_sdk_auth=true)" ${current_cluster}/lma/site-values.yaml

        git config --global user.name "tks"
        git config --global user.email "tks@sktelecom.com"

        if [[ `git status --porcelain` ]]; then
          log "INFO" "##### commit changes on ${current_cluster} to use s3"
          cmessage="changes on ${current_cluster} to use s3"
          git add ${current_cluster}/lma/site-values.yaml
          git commit -m "change loki and thanos endpoints. (by set-primary workflow)" -m "$cmessage"
          git push
        else
          log "INFO" "No change on the cluster ${current_cluster}"
        fi

      env:
      - name: OBJECT_STORE
        value: "{{workflow.parameters.object_store}}"
      envFrom:
      - secretRef:
          name: "git-svc-token"
      volumeMounts:
      - name: kubeconfig-adm
        mountPath: "/kube"
    volumes:
    activeDeadlineSeconds: 900

  - name: sub-sync-organization-changes
    inputs:
      parameters:
      - name: primary_cluster
      - name: member_clusters
    container:
      name: logging-thanos-store-changer
      image: harbor.taco-cat.xyz/tks/shyaml_jq_yq_kubectl_python:3.11
      command:
      - /bin/bash
      - '-c'
      - |
        #/bin/bash

        set -ex

        function log() {
          level=$1
          msg=$2
          date=$(date '+%F %H:%M:%S')
          echo "[$date] $level  $msg"
        }

        current_cluster={{workflow.parameters.cluster_id}}
        primary_cluster={{inputs.parameters.primary_cluster}}
        member_clusters="{{inputs.parameters.member_clusters}}"
        # empty_char=

        if [ -z ${primary_cluster} ] || [[ "${primary_cluster}" == "$empty_char" ]]; then
          primary_cluster=${current_cluster}
        fi

        #################
        # Gather endpoints
        #################
        # Use AWS STS temporary security credential if multi-tenancy
        eplist=\"thanos-storegateway:10901\"
        for member in $member_clusters
        do
          # Thanos Endpoints
          kube_secret=$(kubectl get secret -n ${member} ${member}-tks-kubeconfig -o jsonpath="{.data.value}" | base64 -d)
          cat <<< "$kube_secret" > kubeconfig
          PROMETHEUS_URL=$(kubectl get secret -n ${member} tks-endpoint-secret -o jsonpath='{.data.prometheus}'| base64 -d )
          if [[ "$PROMETHEUS_URL" != "" ]]; then
            eplist="${eplist}, \"${PROMETHEUS_URL}\""
          elif [ `kubectl --kubeconfig=kubeconfig get svc -n lma lma-thanos-external --ignore-not-found=true | grep -v NAME | wc -l ` -eq 1 ]; then
            while [ -z $(kubectl --kubeconfig=kubeconfig get svc -n lma lma-thanos-external -o jsonpath="{.status.loadBalancer.ingress[*].hostname}") ]
            do
              if [ "$(kubectl --kubeconfig=kubeconfig get svc -n lma lma-thanos-external -o jsonpath="{.spec.type}")" -neq "LoadBalancer" ]; then
                log "FAIL" "A service for the thanos-sidcar in ${member} is not cofigured properly.(No LoadBalancer)"
                exit -1
              fi

              echo "Waiting for generating the loadbalancer of Prometheus thanos-sidecar(3s)"
              sleep 3
            done

            eplist="${eplist}, \"$(kubectl --kubeconfig=kubeconfig get svc -n lma lma-thanos-external -o jsonpath="{.status.loadBalancer.ingress[0].hostname}"):$(kubectl --kubeconfig=kubeconfig get svc -n lma lma-thanos-external -o jsonpath="{.spec.ports[0].port}")\""
          else
            log "WARN" "Cluster(${member}) has no prometheus sidecar"
          fi
        done

        cp /kube/value ~/kubeconfig_adm
        export KUBECONFIG=~/kubeconfig_adm

        if [[ "$OBJECT_STORE" == "s3" ]]; then
          #################
          # Gather iams
          #################
          # Use AWS STS temporary security credential if multi-tenancy
          if [ -z $CLOUD_ACCOUNT_ID ]; then
            iamPrefix="arn:aws:iam::$(kubectl  get secret -n argo awsconfig-secret -ojsonpath='{.data.AWS_ACCOUNT_ID}' | base64 -d):role"
          else
            ROLE_ARN=$(kubectl get awsri $CLOUD_ACCOUNT_ID-account-role -ojsonpath='{.spec.roleARN}')
            iamPrefix=${ROLE_ARN%/*}
          fi
          
          for member in $member_clusters; do
            if [ `kubectl get AWSManagedMachinePool -n ${member} ${member}-mp-taco --ignore-not-found=true | grep -v NAME | wc -l ` -eq 1 ]; then
              if [ -z $iamRoles ]; then
                iamRoles="\"${iamPrefix}/$(kubectl get AWSManagedMachinePool -n ${member} ${member}-mp-taco  -o jsonpath='{.spec.roleName}')\""
              else
                iamRoles="${iamRoles}, \"${iamPrefix}/$(kubectl get AWSManagedMachinePool -n ${member} ${member}-mp-taco  -o jsonpath='{.spec.roleName}')\""
              fi
            elif [ `kubectl get AWSMachinePool -n ${member} ${member}-mp-taco --ignore-not-found=true | grep -v NAME | wc -l ` -eq 1 ]; then
              if [ -z $iamRoles ]; then
                iamRoles="\"${iamPrefix}/nodes.cluster-api-provider-aws.sigs.k8s.io\""
              else
                iamRoles="${iamRoles}, \"${iamPrefix}/nodes.cluster-api-provider-aws.sigs.k8s.io\""
              fi
            else
              log "WARN" "Cluster(${member}) has no node role for taco-lma"
            fi
          done
        fi

        #################
        # updates
        #################
        GIT_ACCOUNT={{workflow.parameters.github_account}}
        if  [[ $GIT_SVC_URL == https://* ]]; then
          repository_url=https://${TOKEN//[$'\t\r\n ']}@${GIT_SVC_URL/http:\/\//}/${GIT_ACCOUNT}/${primary_cluster}
        else
          repository_url=http://${TOKEN//[$'\t\r\n ']}@${GIT_SVC_URL/http:\/\//}/${GIT_ACCOUNT}/${primary_cluster}
        fi

        git clone ${repository_url}
        cd ${primary_cluster}
        git config --global user.name "tks"
        git config --global user.email "tks@sktelecom.com"

        yq -i e  ".global.thanosQueryStores=[${eplist}]" ${primary_cluster}/lma/site-values.yaml

        if [[ `git status --porcelain` ]]; then
          log "INFO" "##### commit changes endpoints for the thanos query on ${primary_cluster} "
          cmessage="changes endpoints for the thanos query on ${primary_cluster}"
          git add ${primary_cluster}/lma/site-values.yaml
          git commit -m "change thanos-query stores. (by set-primary workflow)" -m "$cmessage"
          git push
          echo ${primary_cluster} > /mnt/out/changed.txt
        else
          log "INFO" "No change on the cluster ${primary_cluster}"
        fi

        if [[ "$OBJECT_STORE" == "s3" ]]; then
          yq -i e  ".global.tksIamRoles=[${iamRoles}]" ${primary_cluster}/lma/site-values.yaml

          if [[ `git status --porcelain` ]]; then
            log "INFO" "##### commit changes iamRoles for the s3 on ${primary_cluster} "
            cmessage="changes iamRoles for the s3 on ${primary_cluster}"
            git add ${primary_cluster}/lma/site-values.yaml
            git commit -m "change iamRoles(s3). (by set-primary workflow)" -m "$cmessage"
            git push
            echo ${primary_cluster} > /mnt/out/changed.txt
          else
            log "INFO" "(iamRoles) No change on the cluster ${primary_cluster}"
          fi
        fi

        cd -
        rm -rf ${primary_cluster}

      envFrom:
      - secretRef:
          name: "git-svc-token"
      env:
      - name: CLUSTER_ID
        value: "{{workflow.parameters.cluster_id}}"
      - name: CLOUD_ACCOUNT_ID
        value: "{{workflow.parameters.cloud_account_id}}"
      - name: OBJECT_STORE
        value: "{{workflow.parameters.object_store}}"
      volumeMounts:
      - name: kubeconfig-adm
        mountPath: "/kube"
      - name: out
        mountPath: /mnt/out
    volumes:
    - name: out
      emptyDir: {}
    outputs:
      parameters:
      - name: changed_primary_id
        valueFrom:
          path: /mnt/out/changed.txt
          default: "NONE"
    activeDeadlineSeconds: 900

  - name: sub-remove-individual-loki-and-grafana
    inputs:
      parameters:
      - name: target-clusters
    container:
      name: delete-loki
      image: harbor.taco-cat.xyz/tks/hyperkube:v1.18.6
      command:
      - /bin/bash
      - '-c'
      - |
        # log into Argo CD server
        ./argocd login $ARGO_SERVER --plaintext --insecure --username $ARGO_USERNAME \
        --password $ARGO_PASSWORD

        for cid in {{inputs.parameters.target-clusters}}
        do
          for app in loki grafana
          do
            APP=${cid}-${app}

            # Pre-check: validate if the app exists
            if ! (./argocd app list --output name | grep -E "^$APP$"); then
              echo "No such app: $APP. Skipping app removal.."
              exit 1
            fi
          done
        done

        for cid in {{inputs.parameters.target-clusters}}
        do
          for app in loki grafana
          do
            APP=${cid}-${app}
            echo "Found app '$APP'. Start deleting it.."
            ./argocd app delete $APP --cascade -y

            while (./argocd app list --output name | grep -E "^$APP$" )
            do
              echo "Waiting 20 secs for the app to be deleted.."
              sleep 20
            done

            echo "App '$APP' have been deleted!"
          done
        done

      envFrom:
        - secretRef:
            name: "decapod-argocd-config"
    activeDeadlineSeconds: 900

  - name: sub-get-cluster
    synchronization:
      mutex:
        name: primary-cluster-{{workflow.parameters.organization_id}}
    volumes:
    - name: out
      emptyDir: {}
    script:
      image: harbor.taco-cat.xyz/tks/centos-tks-api:v1.0
      command: ["python"]
      env:
      - name: PYTHONPATH
        value: "/opt/protobuf/:/opt/rh/rh-python38/root/lib/python3.8/site-packages/:/opt/app-root/lib/python3.8/site-packages/"
      - name: TKS_API_URL
        value: "{{workflow.parameters.tks_api_url}}"
      envFrom:
      - secretRef:
          name: "tks-api-secret"
      volumeMounts:
      - name: out
        mountPath: /mnt/out
      source: |
        import sys
        import requests
        import json
        import os

        TKS_API_URL = "{{workflow.parameters.tks_api_url}}"
        ORGANIZATION_ID = "{{workflow.parameters.organization_id}}"
        CLUSTER_ID = "{{workflow.parameters.cluster_id}}"

        def getToken() :
          data = {
              'organizationId' : os.environ['ORGANIZATION_ID'],
              'accountId': os.environ['ACCOUNT_ID'],
              'password' : os.environ['PASSWORD']
          }

          res = requests.post(TKS_API_URL+"/api/1.0/auth/login", json = data )
          if res.status_code != 200 :
              return ''
          resJson = res.json()
          return resJson['user']['token']

        # get primary organization id
        res = requests.get(TKS_API_URL+"/api/1.0/organizations/" + ORGANIZATION_ID, headers={"Authorization": "Bearer " + getToken(), "Content-Type" : "application/json"} )
        if res.status_code != 200 :
          sys.exit('Failed to get organization')

        print(res.text)
        organization = res.json()['organization']

        with open("/mnt/out/primary_cluster.txt", "w") as f:
          # primary_cluster = json.dumps(organization.get('phone')) # for test without db manifulation
          primary_cluster = json.dumps(organization.get('primaryClusterId'))
          print('Primary cluster: ', primary_cluster)
          f.write(primary_cluster)

        # get cluster ids
        res = requests.get(TKS_API_URL+"/api/1.0/clusters?organizationId=" + ORGANIZATION_ID, headers={"Authorization": "Bearer " + getToken(), "Content-Type" : "application/json"} )
        if res.status_code != 200 :
          sys.exit('Failed to get clusters')

        # print(res.text)
        clusters = res.json()['clusters']

        with open("/mnt/out/member_clusters.txt", "w") as f:
          for cluster in clusters:
            if cluster['status']=="RUNNING":
              print('Member cluster: ',cluster['id'])
              f.write(cluster['id']+' ')

    outputs:
      parameters:
      - name: primary_cluster
        valueFrom:
          path: /mnt/out/primary_cluster.txt
      - name: member_clusters
        valueFrom:
          path: /mnt/out/member_clusters.txt

    activeDeadlineSeconds: 900
    retryStrategy:
      limit: 2

  - name: sub-set-primary-cluster-on-tks-info
    inputs:
      parameters:
      - name: cluster_id
    script:
      image: harbor.taco-cat.xyz/tks/centos-tks-api:v1.0
      command: ["python"]
      env:
      - name: PYTHONPATH
        value: "/opt/protobuf/:/opt/rh/rh-python38/root/lib/python3.8/site-packages/:/opt/app-root/lib/python3.8/site-packages/"
      - name: TKS_API_URL
        value: "{{workflow.parameters.tks_api_url}}"
      envFrom:
      - secretRef:
          name: "tks-api-secret"
      source: |
        import sys
        import requests
        import json
        import os

        TKS_API_URL = "{{workflow.parameters.tks_api_url}}"
        ORGANIZATION_ID = "{{workflow.parameters.organization_id}}"
        CLUSTER_ID = "{{inputs.parameters.cluster_id}}"

        def getToken() :
          data = {
              'organizationId' : os.environ['ORGANIZATION_ID'],
              'accountId': os.environ['ACCOUNT_ID'],
              'password' : os.environ['PASSWORD']
          }

          res = requests.post(TKS_API_URL+"/api/1.0/auth/login", json = data )
          if res.status_code != 200 :
              return ''
          resJson = res.json()
          return resJson['user']['token']

        # set primary cluster with this cluster
        # api/1.0/organizations/c3c16juq5/primary-cluster
        res = requests.patch(TKS_API_URL+"/api/1.0/organizations/"+ ORGANIZATION_ID + "/primary-cluster",
                              headers={"Authorization": "Bearer " + getToken(), "Content-Type" : "application/json"},
                              data=json.dumps({"primaryClusterId": CLUSTER_ID})
                            )
        if res.status_code != 200 :
          print(res.reason)
          sys.exit('Failed to set primary-cluster at organization')

    activeDeadlineSeconds: 900
    retryStrategy:
      limit: 2

  - name: sub-remove-objects-in-buckets
    container:
      name: remove-objects-in-buckets
      image: harbor.taco-cat.xyz/tks/python_kubectl_helm_argo:v1.0.1
      command:
      - /bin/bash
      - -exc
      - |
        #/bin/bash
        set -ex

        kubectl --kubeconfig /kube/value get secret -n ${cluster_id} ${cluster_id}-tks-kubeconfig -o jsonpath="{.data.value}" | base64 -d > ~/kubeconfig
        export KUBECONFIG=~/kubeconfig

        kubectl delete job -n lma empty-buckets-before-delete-them --ignore-not-found=true
        cat <<EOF | kubectl apply -f -
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: empty-buckets-before-delete-them
          namespace: lma
          annotations:
            "helm.sh/hook": pre-delete
            "helm.sh/hook-delete-policy": hook-succeeded
        spec:
          template:
            spec:
              containers:
              - name: remove-objects-in-buckets
                image: harbor.taco-cat.xyz/tks/aws-cli:2.13.5
                command:
                - /bin/bash
                - '-c'
                - |
                  #/bin/bash
                  set -ex

                  aws s3 rm s3://${cluster_id}-tks-thanos --recursive
                  aws s3 rm s3://${cluster_id}-tks-loki --recursive
                
              restartPolicy: Never
          backoffLimit: 4
        EOF

        while [ $(kubectl get -n lma job empty-buckets-before-delete-them -o jsonpath='{.status.conditions[0].type}'|wc -c) -eq 0 ]; do
          echo "A job is not completed. wait 10 seconds...."
          sleep 10
        done

        if [[ $(kubectl get -n lma job empty-buckets-before-delete-them -o jsonpath='{.status.conditions[0].type}') == "Complete" ]]; then
          exit 0
        else
          exit -1
        fi

      envFrom:
      - secretRef:
          name: "decapod-argocd-config"
      env:
      - name: cluster_id
        value: "{{workflow.parameters.cluster_id}}"
      volumeMounts:
      - name: kubeconfig-adm
        mountPath: "/kube"

    activeDeadlineSeconds: 120
    retryStrategy:
      limit: 2
