apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: aws-cluster-autoscaler-iam
  namespace: argo
spec:
  entrypoint: createIAMRole
  arguments:
    parameters:
    - name: cluster_id
      value: "Cc81dd656"
    - name: cloud_account_id # will be not NULL if the cluster is multitenancy
      value: "NULL"

  volumes:
  - name: awsconfig
    secret:
      secretName: awsconfig-secret
  - name: kubeconfig-adm
    secret:
      secretName: tks-admin-kubeconfig-secret

  templates:
  - name: createIAMRole
    activeDeadlineSeconds: 1800
    inputs:
      parameters:
      - name: cloud_account_id
    container:
      image: harbor.taco-cat.xyz/tks/tks-aws:v1.1.0
      command:
      - /bin/bash
      - -exc
      - |
        cp /kube/value kubeconfig_adm
        export KUBECONFIG=kubeconfig_adm
        mkdir ~/.aws
        cp /aws/* ~/.aws/

        # Use AWS STS temporary security credential if multi-tenancy
        if [ "$CLOUD_ACCOUNT_ID" != "NULL" ]; then
          ROLE_ARN=$(kubectl get awsri $CLOUD_ACCOUNT_ID-account-role -ojsonpath='{.spec.roleARN}')
          aws sts assume-role --role-arn $ROLE_ARN --role-session-name "TKS-ClusterAutoscaler-$CLUSTER_ID" --output json | tee ~/assume-role-sts-credential.txt
          export AWS_ACCESS_KEY_ID=$(cat ~/assume-role-sts-credential.txt | jq -r '.Credentials.AccessKeyId')
          export AWS_SECRET_ACCESS_KEY=$(cat ~/assume-role-sts-credential.txt | jq -r '.Credentials.SecretAccessKey')
          export AWS_SESSION_TOKEN=$(cat ~/assume-role-sts-credential.txt | jq -r '.Credentials.SessionToken')

          ROLE_ARN_REMOVED_SUFFIX=${ROLE_ARN%:*}
          AWS_ACCOUNT_ID=${ROLE_ARN_REMOVED_SUFFIX#*::}
        else
          AWS_ACCOUNT_ID=$(kubectl  get secret -n argo awsconfig-secret -ojsonpath='{.data.AWS_ACCOUNT_ID}' | base64 -d)
        fi

        echo "{
            \"Version\": \"2012-10-17\",
            \"Statement\": [
                {
                    \"Effect\": \"Allow\",
                    \"Action\": [
                        \"autoscaling:SetDesiredCapacity\",
                        \"autoscaling:TerminateInstanceInAutoScalingGroup\"
                    ],
                    \"Resource\": \"*\",
                    \"Condition\": {
                        \"StringEquals\": {
                            \"aws:ResourceTag/k8s.io/cluster-autoscaler/enabled\": \"true\",
                            \"aws:ResourceTag/k8s.io/cluster-autoscaler/$CLUSTER_ID\": \"owned\"
                        }
                    }
                },
                {
                    \"Effect\": \"Allow\",
                    \"Action\": [
                        \"autoscaling:DescribeAutoScalingInstances\",
                        \"autoscaling:DescribeAutoScalingGroups\",
                        \"autoscaling:DescribeScalingActivities\",
                        \"ec2:DescribeLaunchTemplateVersions\",
                        \"autoscaling:DescribeTags\",
                        \"autoscaling:DescribeLaunchConfigurations\",
                        \"ec2:DescribeInstanceTypes\"
                    ],
                    \"Resource\": \"*\"
                }
            ]
        }" > iam_policy.json
        aws iam create-policy --policy-name cluster-autoscaler-$CLUSTER_ID --policy-document file://iam_policy.json

        oidc_id=$(aws eks describe-cluster --name $CLUSTER_ID --query "cluster.identity.oidc.issuer" --output text | cut -d '/' -f 5)
        aws iam list-open-id-connect-providers | grep $oidc_id | cut -d "/" -f4
        eksctl utils associate-iam-oidc-provider --cluster $CLUSTER_ID --approve

        eksctl create iamserviceaccount \
          --name cluster-autoscaler \
          --namespace kube-system \
          --cluster $CLUSTER_ID \
          --role-name "cluster-autoscaler-$CLUSTER_ID" \
          --attach-policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/cluster-autoscaler-$CLUSTER_ID \
          --override-existing-serviceaccounts \
          --approve

      env:
      - name: CLUSTER_ID
        value: "{{workflow.parameters.cluster_id}}"
      - name: CLOUD_ACCOUNT_ID
        value: "{{workflow.parameters.cloud_account_id}}"
      volumeMounts:
      - name: kubeconfig-adm
        mountPath: "/kube"
      - name: awsconfig
        mountPath: "/aws"

  - name: deleteIAMRole
    activeDeadlineSeconds: 1800
    inputs:
      parameters:
      - name: cloud_account_id
    container:
      image: harbor.taco-cat.xyz/tks/tks-aws:v1.1.0
      command:
      - /bin/bash
      - -exc
      - |
        cp /kube/value kubeconfig_adm
        export KUBECONFIG=kubeconfig_adm
        mkdir ~/.aws
        cp /aws/* ~/.aws/

        # Use AWS STS temporary security credential if multi-tenancy
        if [ "$CLOUD_ACCOUNT_ID" != "NULL" ]; then
          ROLE_ARN=$(kubectl get awsri $CLOUD_ACCOUNT_ID-account-role -ojsonpath='{.spec.roleARN}')
          aws sts assume-role --role-arn $ROLE_ARN --role-session-name "TKS-ClusterAutoscaler-$CLUSTER_ID" --output json | tee ~/assume-role-sts-credential.txt
          export AWS_ACCESS_KEY_ID=$(cat ~/assume-role-sts-credential.txt | jq -r '.Credentials.AccessKeyId')
          export AWS_SECRET_ACCESS_KEY=$(cat ~/assume-role-sts-credential.txt | jq -r '.Credentials.SecretAccessKey')
          export AWS_SESSION_TOKEN=$(cat ~/assume-role-sts-credential.txt | jq -r '.Credentials.SessionToken')

          ROLE_ARN_REMOVED_SUFFIX=${ROLE_ARN%:*}
          AWS_ACCOUNT_ID=${ROLE_ARN_REMOVED_SUFFIX#*::}
        else
          AWS_ACCOUNT_ID=$(kubectl  get secret -n argo awsconfig-secret -ojsonpath='{.data.AWS_ACCOUNT_ID}' | base64 -d)
        fi

        eksctl delete iamserviceaccount --cluster $CLUSTER_ID --name cluster-autoscaler --namespace kube-system

        oidc_id=$(aws eks describe-cluster --name $CLUSTER_ID --query "cluster.identity.oidc.issuer" --output text | cut -d '/' -f 5)
        aws iam delete-open-id-connect-provider --open-id-connect-provider-arn arn:aws:iam::$AWS_ACCOUNT_ID:$oidc_id

        aws iam detach-role-policy --role-name cluster-autoscaler-$CLUSTER_ID --policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/cluster-autoscaler-$CLUSTER_ID
        aws iam delete-role --role-name cluster-autoscaler-$CLUSTER_ID

      env:
      - name: CLUSTER_ID
        value: "{{workflow.parameters.cluster_id}}"
      - name: CLOUD_ACCOUNT_ID
        value: "{{workflow.parameters.cloud_account_id}}"
      volumeMounts:
      - name: kubeconfig-adm
        mountPath: "/kube"
      - name: awsconfig
        mountPath: "/aws"
